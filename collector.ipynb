{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5202bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import json, time, sys\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "video_collected = []\n",
    "\n",
    "def formatComment(comment):\n",
    "    doc = {}\n",
    "    doc['id']=comment['id']\n",
    "    doc['comment'] = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "    doc['user'] = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "    try:\n",
    "        doc['userId'] = comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value']\n",
    "    except:\n",
    "        doc['userId'] = \"\"\n",
    "    doc['likeCount'] = comment['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "    doc['publishedAt']=comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "    doc['totalReplyCount'] = comment['snippet']['totalReplyCount']\n",
    "    doc['videoId'] = comment['snippet']['videoId']\n",
    "    doc['parentId'] = \"\"\n",
    "    return doc\n",
    "\n",
    "def formatReply(reply):\n",
    "    doc = {}\n",
    "    doc['id']=reply['id']\n",
    "    doc['comment'] = reply['snippet']['textOriginal']\n",
    "    doc['user'] = reply['snippet']['authorDisplayName']\n",
    "    try:\n",
    "        doc['userId'] = reply['snippet']['authorChannelId']['value']\n",
    "    except:\n",
    "        doc['userId'] = \"\"\n",
    "    doc['likeCount'] = reply['snippet']['likeCount']\n",
    "    doc['publishedAt']=reply['snippet']['publishedAt']\n",
    "    doc['totalReplyCount'] = 0\n",
    "    doc['videoId'] = reply['snippet']['videoId']\n",
    "    doc['parentId'] = reply['snippet']['parentId']\n",
    "    return doc\n",
    "\n",
    "class Collector:\n",
    "    def __init__(self,collected_videos=set()):\n",
    "        with open('config.json') as config:\n",
    "            config=json.load(config)\n",
    "            keys = config['api_keys']\n",
    "        self.youtubes = [build('youtube', 'v3', developerKey=key) for key in keys]\n",
    "        self.youtube = self.youtubes[0]\n",
    "        self.current_queries = 0\n",
    "        self.collected_videos = collected_videos\n",
    "        \n",
    "        \n",
    "    def roll(self):\n",
    "        try:\n",
    "            self.youtube = self.youtubes[self.current_queries//9000]\n",
    "        except:\n",
    "            print('Quota exceeded,sleeping')\n",
    "            time.sleep(60*60*24)\n",
    "            self.youtube=self.youtubes[0]\n",
    "            self.current_queries = 0\n",
    "    \n",
    "    #function to get the top videos for each category according to a sorting method\n",
    "    def getVideos(self, categories,method='relevance'):\n",
    "        videos_metadata=[]\n",
    "        part=[\"id\",\"snippet\"]\n",
    "        for category in categories:\n",
    "            videos = []\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    resp = self.youtube.search().list(regionCode='us',part='snippet',maxResults=50,type='video'\n",
    "                                                    ,order=method,relevanceLanguage='en',videoCategoryId=category)\n",
    "                    result = resp.execute()\n",
    "                    self.current_queries+=100\n",
    "                    self.roll()\n",
    "                    break\n",
    "                except:#service unavailable\n",
    "                    time.sleep(60)\n",
    "            for item in result['items']:\n",
    "                if item['id']['videoId'] in self.collected_videos:\n",
    "                    continue\n",
    "                formattedResult = {}\n",
    "                formattedResult['videoId'] = item['id']['videoId']\n",
    "                formattedResult['publishedAt']=item['snippet']['publishedAt']\n",
    "                formattedResult['title']=item['snippet']['title']\n",
    "                formattedResult['description']=item['snippet']['description']\n",
    "                formattedResult['channelTitle']=item['snippet']['channelTitle']\n",
    "                formattedResult['channelId']=item['snippet']['channelId']\n",
    "                formattedResult['liveBroadCastContent'] = (item['snippet']['liveBroadcastContent']!='none')\n",
    "                formattedResult['category'] = category\n",
    "                formattedResult['method'] = method\n",
    "                videos.append(formattedResult)\n",
    "                self.collected_videos.add(item['videoId'])\n",
    "            while result:\n",
    "                if (len(videos) >= 1000) or ('nextPageToken' not in result):\n",
    "                    break\n",
    "                while True:\n",
    "                    try:\n",
    "                        resp = self.youtube.search().list(regionCode='us',pageToken=result['nextPageToken'],part='snippet',maxResults=50,type='video'\n",
    "                                                    ,order=method,relevanceLanguage='en',videoCategoryId=category)\n",
    "                        result = resp.execute()\n",
    "                        self.current_queries+=100\n",
    "                        self.roll()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(60)\n",
    "                for item in result['items']:\n",
    "                    if item['id']['videoId'] in self.collected_videos:\n",
    "                        continue\n",
    "                    formattedResult = {}\n",
    "                    formattedResult['videoId'] = item['id']['videoId']\n",
    "                    formattedResult['publishedAt']=item['snippet']['publishedAt']\n",
    "                    formattedResult['title']=item['snippet']['title']\n",
    "                    formattedResult['description']=item['snippet']['description']\n",
    "                    formattedResult['channelTitle']=item['snippet']['channelTitle']\n",
    "                    formattedResult['channelId']=item['snippet']['channelId']\n",
    "                    formattedResult['liveBroadCastContent'] = (item['snippet']['liveBroadcastContent']!='none')\n",
    "                    formattedResult['category'] = category\n",
    "                    formattedResult['method'] = method \n",
    "                    videos.append(formattedResult)\n",
    "                    self.collected_videos.add(item['videoId'])\n",
    "            videos_metadata.extend(videos)\n",
    "        return videos_metadata\n",
    "    \n",
    "    #function to get video metadata and view/like/comment counts\n",
    "    def getVideoStatistics(self,videos):\n",
    "        for video in videos:\n",
    "            print(video['videoId'])\n",
    "            while True:\n",
    "                try:\n",
    "                    resp = self.youtube.videos().list(part=['snippet','contentDetails','topicDetails','statistics'],id=video['videoId'])\n",
    "                    result = resp.execute()\n",
    "                    self.current_queries+=1\n",
    "                    self.roll()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    time.sleep(60)\n",
    "            for item in result['items']:\n",
    "                try:\n",
    "                    video['tags'] = item['snippet']['tags']\n",
    "                except:\n",
    "                    video['tags'] = []\n",
    "                try:\n",
    "                    video['language'] = item['snippet']['defaultLanguage']\n",
    "                except:\n",
    "                    video['language'] = ''\n",
    "                video['duration'] = item['contentDetails']['duration']\n",
    "                try:\n",
    "                    video['topics'] = item['topicDetails']['topicCategories']\n",
    "                except:\n",
    "                    video['topics'] = []\n",
    "                video.update(item['statistics'])\n",
    "        return videos\n",
    "    \n",
    "    #function to get channel metadata\n",
    "    def getChannelStatistics(self,channels):\n",
    "        channels_metadata=[]\n",
    "        for channelId in channels:\n",
    "            while True:\n",
    "                try:\n",
    "                    resp= self.youtube.channels().list(part=['id','statistics','snippet'],id=channelId)\n",
    "                    result = resp.execute()\n",
    "                    self.current_queries+=1\n",
    "                    self.roll()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    raise(e)\n",
    "                    time.sleep(60)\n",
    "            if 'items' not in result:\n",
    "                continue\n",
    "            channel_data = {}\n",
    "            channel_data['title']=result['items'][0]['snippet']['title']\n",
    "            channel_data['id'] = result['items'][0]['id']\n",
    "            channel_data['viewCount'] = result['items'][0]['statistics']['viewCount']\n",
    "            try:\n",
    "                channel_data['subscriberCount'] = result['items'][0]['statistics']['subscriberCount']\n",
    "            except:\n",
    "                channel_data['subscriberCount'] = None\n",
    "            try:\n",
    "                channel_data['description'] = result['items'][0]['snippet']['description']\n",
    "            except:\n",
    "                channel_data['description'] = ''\n",
    "            try:\n",
    "                channel_data['publishedAt'] = result['items'][0]['snippet']['publishedAt']\n",
    "            except:\n",
    "                channel_data['publishedAt'] = None\n",
    "            channel_data['videoCount'] = result['items'][0]['statistics']['videoCount']\n",
    "            channels_metadata.append(channel_data)\n",
    "        return channels_metadata\n",
    "\n",
    "    #function to backup currently collected videos\n",
    "    def backup(self):\n",
    "        with open('Data/temp_videos.jsonl','a',encoding='utf-8') as file:\n",
    "            for video in self.videos_metadata:\n",
    "                json.dump(video,file,ensure_ascii=False)\n",
    "                file.write('\\n')\n",
    "        with open('Data/backup.txt','w') as backup:\n",
    "            for pageToken,channel in self.page_queue:\n",
    "                backup.write(str(pageToken)+','+str(channel))\n",
    "                backup.write('\\n')\n",
    "        self.videos_metadata = []\n",
    "        self.n_iter=0\n",
    "        \n",
    "    #gets top 50 videos from a channel\n",
    "    def getFromChannel(self,channels,iter_to_backup=10):\n",
    "        self.videos_metadata=[]\n",
    "        part=[\"id\",\"snippet\"]\n",
    "        self.n_iter=0\n",
    "        for channel in channels:\n",
    "            self.n_iter+=1\n",
    "            if self.n_iter%iter_to_backup == 0:\n",
    "                print(f'backing up data at {self.n_iter} iterations')\n",
    "                self.backup()\n",
    "            videos = []\n",
    "            while True:\n",
    "                try:\n",
    "                    resp = self.youtube.search().list(regionCode='us',part='snippet',maxResults=50,type='video'\n",
    "                                            ,order='viewCount',relevanceLanguage='en',channelId=channel['id'])\n",
    "                    result = resp.execute()\n",
    "                    self.current_queries+=100\n",
    "                    self.roll()\n",
    "                    break\n",
    "                except Exception as e:#service unavailable\n",
    "                    print(e)\n",
    "                    time.sleep(60)\n",
    "            for item in result['items']:\n",
    "                formattedResult = {}\n",
    "                formattedResult['videoId'] = item['id']['videoId']\n",
    "                formattedResult['publishedAt']=item['snippet']['publishedAt']\n",
    "                formattedResult['title']=item['snippet']['title']\n",
    "                formattedResult['description']=item['snippet']['description']\n",
    "                formattedResult['channelTitle']=item['snippet']['channelTitle']\n",
    "                formattedResult['channelId']=item['snippet']['channelId']\n",
    "                formattedResult['liveBroadCastContent'] = (item['snippet']['liveBroadcastContent']!='none')\n",
    "                formattedResult['category'] = item['snippet']['categoryId']\n",
    "                formattedResult['method'] = 'from_channel'\n",
    "                videos.append(formattedResult)\n",
    "            if result and 'nextPageToken' in result:\n",
    "                self.page_queue.append((result['nextPageToken'],channel['id']))\n",
    "            self.videos_metadata.extend(videos)\n",
    "\n",
    "        self.backup()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    #gets up to \"limit\" comments for a given video\n",
    "    def getComments(self,videoId,limit=float('inf')):\n",
    "        comments = []\n",
    "        response = self.youtube.commentThreads().list(part=[\"id\",\"snippet\",\"replies\"],videoId=videoId,maxResults=100).execute()\n",
    "        self.current_queries+=1\n",
    "        self.roll()\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                formattedResult = formatComment(item)\n",
    "                comments.append(formattedResult)\n",
    "                if len(comments)%10000 ==0:\n",
    "                    print(f'{len(comments)} collected for {videoId}')\n",
    "                if len(comments)>=limit:\n",
    "                    return comments\n",
    "                if formattedResult['totalReplyCount']>0:\n",
    "                    try:\n",
    "                        for reply in item['replies']['comments']:\n",
    "                            reply = formatReply(reply)\n",
    "                            comments.append(reply)\n",
    "                    except:\n",
    "                        pass\n",
    "            if 'nextPageToken' in response:\n",
    "                while True:\n",
    "                    try:\n",
    "                        response = self.youtube.commentThreads().list(part=[\"id\",\"snippet\",\"replies\"],videoId = videoId,maxResults=100,pageToken=response['nextPageToken']).execute()\n",
    "                        self.current_queries+=1\n",
    "                        self.roll()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(60)\n",
    "            else:\n",
    "                break\n",
    "        return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e9391",
   "metadata": {},
   "source": [
    "# Main collection loop - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Data/categories.json') as file:\n",
    "        categories = json.load(file)\n",
    "except:\n",
    "    categories = youtube.videoCategories().list(part='snippet',regionCode='US')\n",
    "    with open('Data/categories.json','w') as file:\n",
    "        json.dump(categories,file)\n",
    "\n",
    "\n",
    "category_ids = [category['id'] for category in categories]\n",
    "print(f'Collecting data for {len(category_ids)} categories using method')\n",
    "\n",
    "collector = Collector(collected_videos=set())\n",
    "videos=collector.getVideos(category_ids,method='viewCount')\n",
    "print(f'Finished category queries {len(videos)} found')\n",
    "videos = collector.getVideoStatistics(videos)\n",
    "print(f'Finished collecting statistics')\n",
    "with open('Data/video_metadata.jsonl','a',encoding='utf-8') as file:\n",
    "    for line in videos:\n",
    "        json.dump(line,file,ensure_ascii=False)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ff49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    video_collected.append(video['videoId'])\n",
    "    try:\n",
    "        comments=collector.getComments(video['videoId'])\n",
    "    except Exception as e:\n",
    "        if 'disabled comments' in str(e):\n",
    "            print(f'Video {video[\"videoId\"]} has disabled comments')\n",
    "            continue\n",
    "    print(f'Collected {len(comments)} comments for {video[\"videoId\"]}')\n",
    "    with open('Data/comments.jsonl','a',encoding='utf-8') as file:\n",
    "        for comment in comments:\n",
    "            json.dump(comment,file,ensure_ascii=False)\n",
    "            file.write('\\n')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = set([video['channelId'] for video in videos])\n",
    "print(f'Collecting data for {len(channels)} channels')\n",
    "channel_statistics = collector.getChannelStatistics(channels)\n",
    "with open('Data/channel_metadata.jsonl','w',encoding='utf-8') as file:\n",
    "     for channel in channel_statistics:\n",
    "        json.dump(channel,file,ensure_ascii=False)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c10459",
   "metadata": {},
   "source": [
    "# Collection loop - part 2 \n",
    "## Popular videos from collected channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=pd.read_csv('channels_relevant.csv').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07006c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collector.getFromChannel(channels,iter_to_backup=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7188978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only top 3 videos due to time_constraints\n",
    "videos = pd.read_json('Data/temp_videos.jsonl',lines=True)\n",
    "vid = pd.read_json('Data/video_metadata.jsonl',lines=True)\n",
    "videos_to_collect = []\n",
    "for c,df in videos.groupby('channelId'):\n",
    "    df = df[~df['videoId'].isin(vid['videoId'].tolist())]\n",
    "    df = df[:3]\n",
    "    videos_to_collect.append(df.copy())\n",
    "videos_to_collect = pd.concat(videos_to_collect).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_collect = videos_to_collect.to_dict(orient='records')\n",
    "video_metadata = collector.getVideoStatistics(videos_to_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/video_metadata_from_channels.jsonl','w',encoding='utf-8') as file:\n",
    "    for line in video_metadata:\n",
    "        json.dump(line,file,ensure_ascii=False)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f2bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get comments from videos from channels (keep at most 10**5 comments for each video due to time constraints)\n",
    "for video in videos_to_collect['videoId']:\n",
    "    if video in video_collected:\n",
    "        continue\n",
    "    try:\n",
    "        comments=collector.getComments(video)\n",
    "        while comments=='fail':\n",
    "            print('failed once')\n",
    "            comments=collector.getComments(video,limit=10**5)\n",
    "        video_collected.add(video)\n",
    "    except Exception as e:\n",
    "        if 'disabled comments' in str(e):\n",
    "            print(f'Video {video} has disabled comments')\n",
    "            video_collected.add(video)\n",
    "            continue\n",
    "        else:\n",
    "            print(e)\n",
    "    print(f'Collected {len(comments)} comments for {video}')\n",
    "    with open('Data/comments_from_channels.jsonl','a',encoding='utf-8') as file:\n",
    "        for comment in comments:\n",
    "            json.dump(comment,file,ensure_ascii=False)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d9c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
