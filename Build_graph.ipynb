{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4deb3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99056d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_data(video_users,backup_folder):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(video_users.keys())\n",
    "    keys = list(video_users.keys())\n",
    "    for i,video in enumerate(tqdm(keys)):\n",
    "        for j in range(i+1,len(keys)):\n",
    "            other_video=keys[j]\n",
    "            if video==other_video:\n",
    "                continue\n",
    "            weight = len(video_users[video].intersection(video_users[other_video]))/len(video_users[video].union(video_users[other_video]))\n",
    "            if weight >0:\n",
    "                G.add_edge(video,other_video,weight=weight)\n",
    "    pickle.dump(G, open(backup_folder+'/unsigned_graph.pickle', 'wb'))\n",
    "    return G\n",
    "\n",
    "def create_unsigned_graph(backup_folder='/Graph',comment_file='./Data/complete_comments.jsonl'):\n",
    "    current_video = ''\n",
    "    video_users = {}\n",
    "    user_int_mapping = {}\n",
    "    video_int_mapping = {}\n",
    "    user_set = set()\n",
    "    \n",
    "    videos = pd.read_json('Data/video_metadata.jsonl',lines=True)[['videoId','category']]\n",
    "    \n",
    "    for chunk in pd.read_json(comment_file,encoding='utf-8',chunksize=10**6,lines=True):\n",
    "        chunk = pd.merge(chunk,videos,on='videoId')\n",
    "        if current_video == '':\n",
    "            current_video=chunk.iloc[0]['videoId']\n",
    "        for line in chunk.to_dict(orient='records'):\n",
    "            if int(line['category']) >=30:\n",
    "                continue\n",
    "            if current_video != line['videoId']:\n",
    "                video_int_mapping[current_video] = len(video_int_mapping)\n",
    "                video_users[video_int_mapping[current_video]]=user_set\n",
    "                user_set=set()\n",
    "                current_video = line['videoId']\n",
    "                \n",
    "            if line['userId'] not in user_int_mapping:\n",
    "                user_int_mapping[line['userId']] = len(user_int_mapping)\n",
    "            user_set.add(user_int_mapping[line['userId']])\n",
    "    with open(backup_folder+'/user_index','wb') as file:\n",
    "        pickle.dump(user_int_mapping,file)\n",
    "    with open(backup_folder+'/video_index','wb') as file:\n",
    "        pickle.dump(video_int_mapping,file)\n",
    "    with open(backup_folder+'/video_users','wb') as file:\n",
    "        pickle.dump(video_users,file)\n",
    "    \n",
    "    print('finished reading file')\n",
    "    \n",
    "    return build_graph_from_data(video_users,backup_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a319d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_sentiment_data(video_users,user_int_mapping,video_int_mapping):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(video_users.keys())\n",
    "    keys = list(video_users.keys())\n",
    "    for i,video in tqdm(enumerate(keys)):\n",
    "        for j in range(i+1,len(keys)):\n",
    "            other_video=keys[j]\n",
    "            if video==other_video:\n",
    "                continue\n",
    "            intersection = set(video_users[video].keys()).intersection(set(video_users[other_video].keys()))\n",
    "            sentiments = []\n",
    "            for element in intersection:\n",
    "                sentiments.append(np.mean(video_users[video][element])*np.mean(video_users[other_video][element]))\n",
    "            weight = np.mean(sentiments)\n",
    "            if weight != 0:\n",
    "                G.add_edge(video,other_video,weight=weight)\n",
    "    pickle.dump(G, open('Graph/signed/signed_graph.pickle', 'wb'))\n",
    "    return G\n",
    "\n",
    "def create_signed_graph():\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    current_video = ''\n",
    "    video_users = {}\n",
    "    user_int_mapping = {}\n",
    "    video_int_mapping = {}\n",
    "    user_polarities = defaultdict(list)\n",
    "    for chunk in pd.read_json('./Data/complete_comments.jsonl',encoding='utf-8',chunksize=10**6,lines=True):\n",
    "        if current_video == '':\n",
    "            current_video=chunk.iloc[0]['videoId']\n",
    "        for line in chunk.to_dict(orient='records'):\n",
    "            if current_video != line['videoId']:\n",
    "                video_int_mapping[current_video] = len(video_int_mapping)\n",
    "                video_users[video_int_mapping[current_video]]=user_polarities\n",
    "                user_polarities=defaultdict(list)\n",
    "                current_video = line['videoId']\n",
    "                \n",
    "            if line['userId'] not in user_int_mapping:\n",
    "                user_int_mapping[line['userId']] = len(user_int_mapping)\n",
    "            user_polarities[user_int_mapping[line['userId']]].append(analyzer.polarity_scores(line['comment'])['compound'])\n",
    "    with open('Graph/signed/user_index','wb') as file:\n",
    "        pickle.dump(user_int_mapping,file)\n",
    "    with open('Graph/signed/video_index','wb') as file:\n",
    "        pickle.dump(video_int_mapping,file)\n",
    "    with open('Graph/signed/video_users','wb') as file:\n",
    "        pickle.dump(video_users,file)\n",
    "    return build_graph_from_sentiment_data(video_users,user_int_mapping,video_int_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10bd545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading file\n",
      "CPU times: total: 7h 40min 34s\n",
      "Wall time: 14h 47min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "G=create_unsigned_graph('Graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4abcc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8332/8332 [50:13<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26min 45s\n",
      "Wall time: 53min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "S = create_unsigned_graph('Graph/small','./Data/comments.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c1b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
